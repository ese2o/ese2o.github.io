---
layout: post
title: 
related posts:
description: >
sitemap:
    changefreq: daily
    priority: 1.0
hide_last_modified: true
---


# Eval; CRAG: Comprehensive RAG Benchmark

[CRAG: Comprehensive RAG Benchmark](https://arxiv.org/abs/2406.04744) \
2024.07

#### abstract
기존의 데이터셋이 가지는 현실 세계와의 격차를 좁히기 위해, Comprehensive RAG Benchmark (CRAG) 소개
- 사실 기반 질문 응답 벤치마크; factual question answering benchmark
- 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search
  - 지식 그래프: 현실 세계의 개체와 그 관계를 노드와 엣지로 표현한 데이터 구조 
- 메인/마이너를 아우르는 다양한 엔티티 & 시간적 역동성 반영
- 대부분의 LLM은 CRAG에서 34%의 정확도를 달성, RAG를 추가해도 44%
- sota RAG가 63% 달성
- KDD Cup 2024 챌린지의 기초 마련


#### introduction
![](/assets/img/llm/llm12/1.png)

연구에 따르면, GPT-4의 느리게/빠르게 변하는 사실에 대한 질문 응답 정확도는 15% 미만 \
심지어 안정적인 사실에 대해서도 마이너한 엔티티(torso-to-tail; less popular)에 대한 질문 응답 정확도는 35% 미만 \
-> hallucination 해결하는 것이 매우 중요하다. \
RAG가 직면한 문제
- 가장 관련성이 높은 정보를 선택하는 문제
- 응답 지연을 줄이는 문제
- 복잡한 질문에 대해 정보를 종합하는 문제
이를 개선시킬 포괄적인 벤치마크 제시

좋은 QA 벤치마크의 5가지 핵심 요소
- 현실성
- 다양성
- 통찰력
- 신뢰성
- 지속성

CRAG의 기여
1. 데이터셋 제공
- 5개 분야에서 4409개의 QA 쌍, 단순 사실 질문뿐만 아니라 7가지 복잡한 질문 유형을 포함
  - Conditions, Comparison, Aggregation, Multi-hop, Set queries, Post-processing-heavy, False-pemise
2. 평가 메커니즘 제공
- 3가지 평가 과제 설계: 검색 요약, 지식 그래프 및 웹 검색, 전체 RAG 파이프라인
- 환각된 답변과 누락된 답변 구별 (hallucinated answers, missing answers)
- 자동 평가 메커니즘 설계
3. 포괄적인 평가
- 높은 역동성 / 낮은 인기도 / 높은 복잡성을 가진 사실에 대한 질문에서 정확도가 크게 감소

#### 기존 벤치마크와의 비교
![](/assets/img/llm/llm12/2.png)
- 기존에는 주로 웹 검색 결과 또는 지식 그래프 검색 결과만을 고려함
- 신규 LLM 또는 RAG 벤치마크들은 보통 QA 시스템의 특정 기능을 평가하는 데 중점
- 본 연구에서는 단순한 지식 질문에 얼마나 잘 답변하는지 + 더 복잡한 시나리오를 얼마나 잘 처리할 수 있는지 평가

기존 QA 벤치마크: ROUGE 또는 F1과 같은 매칭 기반 평가 지표(matching-based metrics)를 사용하여 응답 품질을 평가 \
ex. ROUGE: 몇 글자 겹치는지 ~ https://supkoon.tistory.com/26 \
이는 extractive 방법에는 효과적이지만, 자유 형식 응답을 생성하는 LLM에는 그다지 효과적이지 않음. \
데이터셋 크기는 MS MARCO와 NQ보다 작지만, 장점:
> comprehensive coverage, realistic testing with mock APIs, dynamic question handling, diverse fact popularity, extensive content beyond Wikipedia, and fast yet reliable evaluation.

#### metrics
- perfect(+1): The response correctly answers the user’s question and contains no hallucinated content.
- (+0.5): The response provides a useful answer to the user’s question but may contain minor errors that do not harm the usefulness of the answer.
- missing(0): The response is “I don’t know”, “I’m sorry I can’t find ...”, a system error such as an empty response, or a request from the system to clarify the original question. (모르면 모른다고 할때)
- incorrect(-1): The response provides wrong or irrelevant information to answer the user’s question.